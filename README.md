# QLoRA vs GPT-2: Budget Fine-Tuning on Google Colab

This repository contains an experiment comparing QLoRA (on LLaMA-3B) and GPT-2 (trained from scratch) for efficient language model fine-tuning on limited hardware using Google Colab.

## Structure

- `notebooks/`: Jupyter notebook implementation
- `images/`: Visualizations and diagrams used in the article
- `results/`: Logs and final PDFs
- `scripts/`: Utility or training scripts (if any)

## Description

This project accompanies a Medium post focused on practical low-resource fine-tuning using 4-bit quantization (QLoRA) and parameter-efficient methods. All diagrams and experiment logs are included for replication and analysis.

## Credits

Visualizations and code created by the author. Refer to the QLoRA paper by Dettmers et al. (2023) for theoretical details.

